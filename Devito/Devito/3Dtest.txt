nohup: ignoring input
filter_sigma: (5, 5, 5)
Operator `Forward` ran in 0.27 s
Operator `Forward` ran in 0.33 s
Operator `Forward` ran in 0.26 s
Operator `Forward` ran in 0.29 s
Operator `Forward` ran in 0.42 s
Operator `Forward` ran in 0.44 s
Operator `Forward` ran in 1.26 s
Operator `Forward` ran in 0.58 s
Operator `Forward` ran in 0.71 s
Operator `Forward` ran in 0.53 s
Operator `Forward` ran in 1.35 s
Operator `Forward` ran in 1.40 s
Operator `Forward` ran in 1.37 s
Operator `Forward` ran in 1.29 s
Operator `Forward` ran in 0.80 s
Operator `Forward` ran in 1.39 s
Operator `Forward` ran in 1.37 s
Operator `Forward` ran in 1.22 s
Operator `Forward` ran in 1.15 s
Operator `Forward` ran in 0.45 s
Operator `Forward` ran in 0.30 s
Operator `Forward` ran in 0.31 s
Operator `Forward` ran in 0.27 s
Operator `Forward` ran in 0.27 s
Operator `Forward` ran in 0.28 s
Operator `Forward` ran in 3.75 s
Operator `Forward` ran in 3.52 s
Operator `Forward` ran in 4.36 s
Operator `Forward` ran in 4.33 s
Operator `Forward` ran in 3.59 s
Operator `Forward` ran in 5.04 s
Operator `Forward` ran in 4.97 s
Operator `Forward` ran in 3.89 s
Operator `Forward` ran in 3.85 s
Operator `Forward` ran in 4.21 s
Operator `Forward` ran in 4.20 s
Operator `Forward` ran in 5.88 s
Operator `Forward` ran in 6.34 s
Operator `Forward` ran in 4.18 s
Operator `Forward` ran in 4.18 s
Operator `Forward` ran in 8.04 s
Operator `Forward` ran in 7.82 s
Operator `Forward` ran in 7.56 s
Operator `Forward` ran in 6.41 s
Operator `Forward` ran in 7.29 s
Operator `Forward` ran in 7.01 s
Operator `Forward` ran in 7.55 s
Operator `Forward` ran in 6.96 s
Operator `Forward` ran in 6.43 s
Operator `Forward` ran in 6.37 s
Operator `Gradient` ran in 2.86 s
Operator `Gradient` ran in 4.62 s
Operator `Gradient` ran in 6.09 s
Operator `Gradient` ran in 7.22 s
Operator `Gradient` ran in 6.22 s
Operator `Gradient` ran in 7.14 s
Operator `Gradient` ran in 8.26 s
Operator `Gradient` ran in 5.97 s
Operator `Gradient` ran in 7.56 s
Operator `Gradient` ran in 8.77 s
Operator `Gradient` ran in 8.09 s
Operator `Gradient` ran in 5.71 s
Operator `Gradient` ran in 9.86 s
Operator `Gradient` ran in 8.97 s
Operator `Gradient` ran in 5.18 s
Operator `Gradient` ran in 6.22 s
Operator `Gradient` ran in 4.89 s
Operator `Gradient` ran in 6.30 s
Operator `Gradient` ran in 9.83 s
Operator `Gradient` ran in 7.98 s
Operator `Gradient` ran in 6.35 s
Operator `Gradient` ran in 2.44 s
Operator `Gradient` ran in 6.51 s
Operator `Gradient` ran in 4.26 s
Operator `Gradient` ran in 2.37 s
Operator `Forward` ran in 6.08 s
Operator `Forward` ran in 5.88 s
Operator `Forward` ran in 6.96 s
Operator `Forward` ran in 6.36 s
Operator `Forward` ran in 6.69 s
Operator `Forward` ran in 8.30 s
Operator `Forward` ran in 8.21 s
Operator `Forward` ran in 7.15 s
Operator `Forward` ran in 7.44 s
Operator `Forward` ran in 9.56 s
Operator `Forward` ran in 8.04 s
Operator `Forward` ran in 5.93 s
Operator `Forward` ran in 7.29 s
Operator `Forward` ran in 8.90 s
Operator `Forward` ran in 7.51 s
Operator `Forward` ran in 6.99 s
Operator `Forward` ran in 13.90 s
Operator `Forward` ran in 10.93 s
Operator `Forward` ran in 11.94 s
Operator `Forward` ran in 12.25 s
Operator `Forward` ran in 11.63 s
Operator `Forward` ran in 10.01 s
Operator `Forward` ran in 12.12 s
Operator `Forward` ran in 12.00 s
Operator `Forward` ran in 11.62 s
Operator `Gradient` ran in 3.49 s
Operator `Gradient` ran in 6.00 s
Operator `Gradient` ran in 7.89 s
Operator `Gradient` ran in 10.15 s
Operator `Gradient` ran in 7.80 s
Operator `Gradient` ran in 9.87 s
Operator `Gradient` ran in 9.54 s
Operator `Gradient` ran in 9.91 s
Operator `Gradient` ran in 8.78 s
Operator `Gradient` ran in 13.91 s
Operator `Gradient` ran in 8.31 s
Operator `Gradient` ran in 8.44 s
Operator `Gradient` ran in 13.37 s
Operator `Gradient` ran in 7.93 s
Operator `Gradient` ran in 8.51 s
Operator `Gradient` ran in 14.12 s
Operator `Gradient` ran in 15.68 s
Operator `Gradient` ran in 8.93 s
Operator `Gradient` ran in 8.90 s
Operator `Gradient` ran in 14.68 s
Operator `Gradient` ran in 9.39 s
Operator `Gradient` ran in 14.94 s
Operator `Gradient` ran in 14.60 s
Operator `Gradient` ran in 12.66 s
Operator `Gradient` ran in 8.90 s
312
312
79.51352858543396 s
79.72654891014099 s
79.87003874778748 s

RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =        15625     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  9.17772D+03    |proj g|=  2.72538D-01

At iterate    1    f=  7.61884D+03    |proj g|=  2.93017D-01

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
15625      1      2  15145     0 15144   2.930D-01   7.619D+03
  F =   7618.8435787306316     

STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 
mprof: Sampling memory every 0.1s
running new process
