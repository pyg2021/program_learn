nohup: ignoring input
Traceback (most recent call last):
  File "/home/pengyaoguang/program_learn/program/shengli/3D_net6_4.py", line 65, in <module>
    y_1=model(x)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 175, in forward
    inputs, module_kwargs = self.scatter(inputs, kwargs, self.device_ids)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 197, in scatter
    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 73, in scatter_kwargs
    scattered_inputs = scatter(inputs, target_gpus, dim) if inputs else []
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 60, in scatter
    res = scatter_map(inputs)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 47, in scatter_map
    return list(zip(*map(scatter_map, obj)))
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 43, in scatter_map
    return Scatter.apply(target_gpus, None, dim, obj)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 96, in forward
    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
  File "/home/pengyaoguang/.conda/envs/pytorch3.9/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 187, in scatter
    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

